{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Project description:\n",
    "    \n",
    "We are an analyst at a big online store. Together with the marketing department, we've compiled a list of hypotheses that may help boost revenue.\n",
    "    \n",
    "#### Description of the data\n",
    "Data used in the first part of the project\n",
    "###### /datasets/hypotheses_us.csv Download dataset\n",
    "* Hypotheses — brief descriptions of the hypotheses\n",
    "* Reach — user reach, on a scale of one to ten\n",
    "* Impact — impact on users, on a scale of one to ten\n",
    "* Confidence — confidence in the hypothesis, on a scale of one to ten\n",
    "* Effort — the resources required to test a hypothesis, on a scale of one to ten. The higher the Effort value, the more resource-intensive the test.\n",
    "    \n",
    "Data used in the second part of the project\n",
    "###### /datasets/orders_us.csv Download dataset\n",
    "* transactionId — order identifier\n",
    "* visitorId — identifier of the user who placed the order\n",
    "* date — of the order\n",
    "* revenue — from the order\n",
    "* group — the A/B test group that the user belongs to\n",
    "##### /datasets/visits_us.csv Download dataset\n",
    "    \n",
    "* date — date\n",
    "* group — A/B test group\n",
    "* visits — the number of visits on the date specified in the A/B test group specified\n",
    "    \n",
    "    \n",
    "### Part 1. Prioritizing Hypotheses\n",
    "The file hypotheses_us.csv contains nine hypotheses on boosting an online store's revenue with Reach, Impact, Confidence, and Effort specified for each.\n",
    "\n",
    "#### The task is to:\n",
    "* Apply the ICE framework to prioritize hypotheses. Sort them in descending order of priority.\n",
    "* Apply the RICE framework to prioritize hypotheses. Sort them in descending order of priority.\n",
    "* Show how the prioritization of hypotheses changes when you use RICE instead of ICE. Provide an explanation for the changes.\n",
    "    \n",
    "### Part 2. A/B Test Analysis\n",
    "We carried out an A/B test and got the results described in the files orders_us.csv and visits_us.csv.\n",
    "    \n",
    "#### The task is to:\n",
    "##### Analyze the A/B test:\n",
    "    \n",
    "* Graph cumulative revenue by group. Make conclusions and conjectures.\n",
    "* Graph cumulative average order size by group. Make conclusions and conjectures.\n",
    "* Graph the relative difference in cumulative average order size for group B compared with group A. Make conclusions and conjectures.\n",
    "* Calculate each group's conversion rate as the ratio of orders to the number of visits for each day. Plot the daily conversion rates of the two groups and describe the difference. Draw conclusions and make conjectures.\n",
    "* Plot a scatter chart of the number of orders per user. Make conclusions and conjectures.\n",
    "* Calculate the 95th and 99th percentiles for the number of orders per user. Define the point at which a data point becomes an anomaly.\n",
    "* Plot a scatter chart of order prices. Make conclusions and conjectures.\n",
    "* Calculate the 95th and 99th percentiles of order prices. Define the point at which a data point becomes an anomaly.\n",
    "* Find the statistical significance of the difference in conversion between the groups using the raw data. Make conclusions and conjectures.\n",
    "* Find the statistical significance of the difference in average order size between the groups using the raw data. Make conclusions and conjectures.\n",
    "* Find the statistical significance of the difference in conversion between the groups using the filtered data. Make conclusions and conjectures.\n",
    "* Find the statistical significance of the difference in average order size between the groups using the filtered data. Make conclusions and conjectures.\n",
    "* Make a decision based on the test results. The possible decisions are: 1. Stop the test, consider one of the groups the leader. 2. Stop the test, conclude that there is no difference between the groups. 3. Continue the test.\n",
    "\n",
    "### Project purpose:\n",
    "Our ultimate purpose is to prioritize hypotheses, launch an A/B test, and analyze the results.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Step 1. Download the data and prepare it for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats as st\n",
    "import datetime as dt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_PATH_1 = 'hypotheses_us.csv'\n",
    "PLATFORM_PATH_1 = '/datasets/hypotheses_us.csv'\n",
    "\n",
    "LOCAL_PATH_2 = 'orders_us.csv'\n",
    "PLATFORM_PATH_2 = '/datasets/orders_us.csv'\n",
    "\n",
    "LOCAL_PATH_3 = 'visits_us.csv'\n",
    "PLATFORM_PATH_3 = '/datasets/visits_us.csv'\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    hypotheses = pd.read_csv(LOCAL_PATH_1,sep=';')\n",
    "except:\n",
    "    hypotheses = pd.read_csv(PLATFORM_PATH_1,sep=';')\n",
    "#-------------------------------------------------    \n",
    "try:\n",
    "    order= pd.read_csv(LOCAL_PATH_2)\n",
    "except:\n",
    "    order= pd.read_csv(PLATFORM_PATH_2)\n",
    "#-----------------------------------------------------\n",
    "try:\n",
    "    visit = pd.read_csv(LOCAL_PATH_3)\n",
    "except:\n",
    "    visit = pd.read_csv(PLATFORM_PATH_3)\n",
    " \n",
    "    \n",
    "#-----------------------------------------------------\n",
    "pd.set_option('max_colwidth', 400)\n",
    "display(hypotheses)\n",
    "display(hypotheses.info())\n",
    "display(hypotheses.isnull().sum())\n",
    "#-----------------------------------\n",
    "display(order.head())\n",
    "display(order.info())\n",
    "display(order.isnull().sum())\n",
    "#--------------------------------------------\n",
    "display(visit .head())\n",
    "display(visit.info())\n",
    "display(visit.isnull().sum())\n",
    "\n",
    "#checking duplicates in dataframe\n",
    "print('duplicate rows:',hypotheses.duplicated().sum())\n",
    "#---------------------------------------------------\n",
    "print('duplicate rows:',order.duplicated().sum())\n",
    "#-------------------------------------------------------------\n",
    "print('duplicate rows:',visit.duplicated().sum())\n",
    "#----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Conclusion:\n",
    "    \n",
    "In the beginning, the datasets have been opened and the general information have been analyzed .There have three dataframe hypotheses,order and visit. The hypotheses data set has 9 rows and 5 columns, The order data set has 1197 rows and 5 columns and The visit data set has 62 rows and 3 columns. There have no missing and duplicate values observed.And all The data types are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the column names in lowecase\n",
    "hypotheses.columns=hypotheses.columns.str.lower()\n",
    "order.columns=order.columns.str.lower()\n",
    "visit.columns=visit.columns.str.lower()\n",
    "pd.set_option('max_colwidth', 400)\n",
    "display(hypotheses.head(1))\n",
    "display(order.head(1))\n",
    "display(visit.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Step 2.Prioritizing Hypotheses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply ICE\n",
    "hypotheses['ICE'] = (hypotheses['impact'] * hypotheses['confidence'])  /hypotheses['effort']\n",
    "pd.set_option('max_colwidth', 400)\n",
    "display(hypotheses[['hypothesis','ICE']].sort_values(by='ICE',ascending=False))\n",
    "#---------------------------------------------------------------------------\n",
    "#Apply RICE\n",
    "hypotheses['RICE'] = (hypotheses['impact'] * hypotheses['confidence']* hypotheses['reach'])  /hypotheses['effort']\n",
    "pd.set_option('max_colwidth', 400)\n",
    "display(hypotheses[['hypothesis','RICE']].sort_values(by='RICE',ascending=False))\n",
    "\n",
    "#----------------------------------------------\n",
    "display(hypotheses.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Show how the prioritization of hypotheses changes when you use RICE instead of ICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the difference\n",
    "ax = hypotheses[['hypothesis','ICE','RICE']].plot(kind='bar',stacked=False, figsize=(8,6))\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height().round()), (p.get_x() * 1.005, p.get_height() * 1.005), rotation=90)\n",
    "    \n",
    "plt.title('ICE vs RICE Prioritization')\n",
    "plt.xlabel('Hypotheses')\n",
    "plt.ylabel('ICE/RICE Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Conclusion:\n",
    "\n",
    "ICE gives 8,0,7 and 6 as the most promissing hypotheses considering index number.For RICE, the most promissing hypotheses are 7,2,0 and 6 considering index number.However, this time index number 7 is in first place.That's because its reach parameter is 10,where the other hypotheses have lower reach values.Different methods change the hypotheses priorities in different level.According to ICE vs RICE graph, top 3 three priotize hypothesis are:\n",
    "\n",
    "1. Launch a promotion that gives users discounts on their birthdays\n",
    "2. Add two new channels for attracting traffic. This will bring 30% more users\n",
    "3. Add a subscription form to all the main pages. This will help you compile a mailing list\n",
    "    \n",
    "Apart from , how strong the hypotheses is, it is important to consider how many users will affect.So 'reach' metric plays a vital role from my understandings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Step 3. A/B Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Graph cumulative revenue by group. Make conclusions and conjectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order['date'] = order['date'].map(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))\n",
    "visit['date'] = visit['date'].map(lambda x: dt.datetime.strptime(x, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove visitors with both groups\n",
    "filtered_list=[order.groupby(['visitorid'])['group'].nunique().reset_index().query('group>1')]\n",
    "display(filtered_list)\n",
    "filtered_order=~order.visitorid.isin(filtered_list)\n",
    "unique_order=order[filtered_order]\n",
    "display(unique_order.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building an array with unique paired date-group values\n",
    "datesGroups = unique_order[['date','group']].drop_duplicates()\n",
    "# getting aggregated cumulative daily data on orders \n",
    "ordersAggregated = datesGroups.apply(lambda x: unique_order[np.logical_and(unique_order['date'] <= x['date'], unique_order['group'] == x['group'])].agg({'date' : 'max', 'group' : 'max', 'transactionid' : pd.Series.nunique, 'visitorid' : pd.Series.nunique, 'revenue' : 'sum'}), axis=1).sort_values(by=['date','group'])\n",
    "display(ordersAggregated.head())\n",
    "# getting aggregated cumulative daily data on visitors  \n",
    "visitorsAggregated = datesGroups.apply(lambda x: visit[np.logical_and(visit['date'] <= x['date'], visit['group'] == x['group'])].agg({'date' : 'max', 'group' : 'max', 'visits' : 'sum'}), axis=1).sort_values(by=['date','group'])\n",
    "\n",
    "# merging the two tables into one and giving its columns descriptive names\n",
    "cumulativeData = ordersAggregated.merge(visitorsAggregated, left_on=['date', 'group'], right_on=['date', 'group'])\n",
    "cumulativeData.columns = ['date', 'group', 'orders', 'buyers', 'revenue', 'visitors']\n",
    "\n",
    "display(cumulativeData.head(5)) \n",
    "\n",
    "# DataFrame with cumulative orders and cumulative revenue by day, group A\n",
    "cumulativeRevenueA = cumulativeData[cumulativeData['group']=='A'][['date','revenue', 'orders']]\n",
    "\n",
    "# DataFrame with cumulative orders and cumulative revenue by day, group B\n",
    "cumulativeRevenueB = cumulativeData[cumulativeData['group']=='B'][['date','revenue', 'orders']]\n",
    "\n",
    "# Plotting the group A revenue graph \n",
    "plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue'], label='A')\n",
    "\n",
    "# Plotting the group B revenue graph \n",
    "plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue'], label='B')\n",
    "\n",
    "plt.legend() \n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.xlabel(\"Time period\")\n",
    "plt.ylabel(\"Cumulative revenue\")\n",
    "plt.title('Group A VS Gropup B cumulative revenue')\n",
    "plt.show()\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion:\n",
    "\n",
    "The cumulative metrics of the B group are consistently higer than those of the A group.Fluctuation in conversion was observed in B groups.It seems like abnormally big orders for B groups affected the results overall. It is most likely to early to make decision based on this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph cumulative average order size by group. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulativeRevenueA['date'], cumulativeRevenueA['revenue']/cumulativeRevenueA['orders'], label='A')\n",
    "plt.plot(cumulativeRevenueB['date'], cumulativeRevenueB['revenue']/cumulativeRevenueB['orders'], label='B')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Time period\")\n",
    "plt.ylabel(\"Cumulative average order size\")\n",
    "plt.title('Group A VS Gropup B cumulative order size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion:\n",
    "\n",
    "The cumulative value of the segments average purchase size is still fluctuating.It is most likely to early to make decision based on this metric.Or we may need to analyse the outliers that are distorting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Graph the relative difference in cumulative average order size for group B compared with group A. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gathering the data into one DataFrame\n",
    "mergedCumulativeRevenue = cumulativeRevenueA.merge(cumulativeRevenueB, left_on='date', right_on='date', how='left', suffixes=['A', 'B'])\n",
    "display(mergedCumulativeRevenue.head(1))\n",
    "\n",
    "# plotting a relative difference graph for the average purchase sizes\n",
    "plt.plot(mergedCumulativeRevenue['date'], (mergedCumulativeRevenue['revenueB']/mergedCumulativeRevenue['ordersB'])/(mergedCumulativeRevenue['revenueA']/mergedCumulativeRevenue['ordersA'])-1)\n",
    "# adding the X axis\n",
    "plt.axhline(y=0, color='black', linestyle='--') \n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Time period\")\n",
    "plt.ylabel(\"Relative difference\")\n",
    "plt.title('Group A VS Gropup B relative difference graph for the average purchase sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion:\n",
    "\n",
    "At several points the difference between the segments spikes. This means there must be some big orders and outliers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate each group's conversion rate as the ratio of orders to the number of visits for each day. Plot the daily conversion rates of the two groups and describe the difference. Draw conclusions and make conjectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating cumulative conversion\n",
    "cumulativeData['conversion'] = cumulativeData['orders']/cumulativeData['visitors']\n",
    "\n",
    "# selecting data on group A \n",
    "cumulativeDataA = cumulativeData[cumulativeData['group']=='A']\n",
    "\n",
    "# selecting data on group B\n",
    "cumulativeDataB = cumulativeData[cumulativeData['group']=='B']\n",
    "\n",
    "# plotting the graphs\n",
    "plt.plot(cumulativeDataA['date'], cumulativeDataA['conversion'], label='A')\n",
    "plt.plot(cumulativeDataB['date'], cumulativeDataB['conversion'], label='B')\n",
    "plt.legend()\n",
    "\n",
    "# setting the axes' scale\n",
    "plt.axis([\"2019-08-01\", \"2019-09-01\", 0, 0.1]) \n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Time period\")\n",
    "plt.ylabel(\"conversion rate\")\n",
    "plt.title('Group A VS Gropup B daily conversion rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion: \n",
    "\n",
    "At the beginning of the test,group A had higher conversion rate but group B gradually gained and established at an almost steady value.The groups were fluctuating around the same value, but then the conversion rate of group A rose before stabilizing, while the conversion rate of group B increased but then stabilized, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot a scatter chart of the number of orders per user. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_2=unique_order.copy()\n",
    "ordersByUsers = (unique_order.drop(['date'], axis=1).groupby(['visitorid','group'], as_index=False).agg({'transactionid': pd.Series.nunique,'revenue': pd.Series.mean}))\n",
    "\n",
    "display(ordersByUsers.head(1))\n",
    "ordersByUsers.columns = ['userid', 'group','orders','revenue']\n",
    "\n",
    "display(ordersByUsers.sort_values(by='orders', ascending=False).head(10))\n",
    "plt.hist(ordersByUsers['orders']) \n",
    "plt.xlabel(\"Orders\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title('Distribution histogram of orders')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "x_values = pd.Series(range(0,len(ordersByUsers)))\n",
    "ax= sns.scatterplot(data=ordersByUsers, x=x_values, y=\"orders\",hue=\"group\", legend=\"full\")\n",
    "ax.set(xlabel='No of users', ylabel='No of orders',title='Orders per user')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion:\n",
    "\n",
    "There are users with 7, 6, 5, 4 orders. That's a lot more than a regular user would place in 4 days.The majority of customers placed only one order. However, a significant share placed two to three apiece.There are indeed a lot of users with two to three orders. The exact share remains unknown, since it's not clear whether we should consider them anomalies or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Calculate the 95th and 99th percentiles for the number of orders per user. Define the point at which a data point becomes an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(ordersByUsers['orders'], [90, 95, 99])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion:  \n",
    "\n",
    "Not more than 5% of users placed more than 2 orders, and 10% of users made more than one.\n",
    "Thus, it would be reasonable to set two or three orders per user as the lower limit for the number of orders and to filter anomalies on that basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot a scatter chart of order prices. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ordersByUsers.sort_values(by='revenue', ascending=False).head(10))\n",
    "plt.hist(ordersByUsers['revenue']) \n",
    "plt.xlabel(\"Revenue\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title('Distribution histogram of average revenue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "x_values = pd.Series(range(0,len(ordersByUsers)))\n",
    "subplot(1,2,1)\n",
    "plt.scatter(x_values, ordersByUsers['revenue']) \n",
    "plt.xlabel(\"No of users\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.title('Revenue per user')\n",
    "\n",
    "subplot(1,2,2)\n",
    "plt.scatter(x_values, ordersByUsers['revenue']) \n",
    "plt.xlabel(\"No of users\")\n",
    "plt.ylabel(\"Revenue\")\n",
    "plt.title('Revenue per user')\n",
    "plt.axis([0, 1100, 0, 2500]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion:\n",
    "\n",
    "\n",
    "There are users with 19920 usd average revenue. That's a lot more than a regular user would buy in 4 days.The majority of customers placed order in the average price range less than 1500. The exact share remains unknown, since it's not clear whether we should consider them anomalies or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Calculate the 95th and 99th percentiles of order prices. Define the point at which a data point becomes an anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.percentile(ordersByUsers['revenue'], [90, 95, 99])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Conclusion:  \n",
    "\n",
    "Not more than 5% of users average order price more than 435 Usd, and 10% of users made more than 281 Usd.\n",
    "Thus, it would be reasonable to set 435 Usd per user as the lower limit for the order price and to filter anomalies on that basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Find the statistical significance of the difference in conversion between the groups using the raw data. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Testing:**\n",
    "Let's compare the sample means for conversion:\n",
    "1. H0  - the sample means have no difference.\n",
    "1. H1  - the sample means are different.\n",
    "1. alpha - 0.05    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersByUsersA = unique_order[unique_order['group']=='A'].groupby('visitorid', as_index=False).agg({'transactionid' : pd.Series.nunique})\n",
    "ordersByUsersA.columns = ['userId', 'orders']\n",
    "\n",
    "ordersByUsersB = unique_order[unique_order['group']=='B'].groupby('visitorid', as_index=False).agg({'transactionid' : pd.Series.nunique})\n",
    "ordersByUsersB.columns = ['userId', 'orders']\n",
    "\n",
    "sampleA = pd.concat([ordersByUsersA['orders'],pd.Series(0, index=np.arange(visit[visit['group']=='A']['visits'].sum() - len(ordersByUsersA['orders'])), name='orders')],axis=0)\n",
    "\n",
    "sampleB = pd.concat([ordersByUsersB['orders'],pd.Series(0, index=np.arange(visit[visit['group']=='B']['visits'].sum() - len(ordersByUsersB['orders'])), name='orders')],axis=0)\n",
    "\n",
    "print(\"{0:.3f}\".format(stats.mannwhitneyu(sampleA, sampleB,alternative= \"two-sided\")[1]))\n",
    "\n",
    "print(\"{0:.3f}\".format(sampleB.mean()/sampleA.mean()-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion: \n",
    "\n",
    "Based on the raw data, there is no difference between groups A and B.\n",
    "The first row of the output gives us the p-value, 0.017, which is greater than 0.05. So we can't reject the null hypothesis that there's  not a statistically significant difference in conversion between the groups. But the relative difference of group B is 13.8% (the second row of the output)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Find the statistical significance of the difference in average order size between the groups using the raw data. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Testing:**\n",
    "Let's compare the sample means for order size:\n",
    "1. H0  - the sample means have no difference.\n",
    "1. H1  - the sample means are different.\n",
    "1. alpha - 0.05    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:.3f}'.format(stats.mannwhitneyu(order[order['group']=='A']['revenue'], order[order['group']=='B']['revenue'],alternative= \"two-sided\")[1]))\n",
    "print('{0:.3f}'.format(order[order['group']=='B']['revenue'].mean()/order[order['group']=='A']['revenue'].mean()-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion:\n",
    "\n",
    "The p-value is considerably higher than 0.05, so there's no reason to reject the null hypothesis and conclude that average order size differs between the groups. Nonetheless, the average order size for group B is smaller than it is for group A and it is almost 25% bigger.So, it is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Find the statistical significance of the difference in conversion between the groups using the filtered data. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersWithManyOrders = pd.concat([ordersByUsersA[ordersByUsersA['orders'] > 2]['userId'], ordersByUsersB[ordersByUsersB['orders'] > 2]['userId']], axis = 0)\n",
    "usersWithExpensiveOrders = unique_order[unique_order['revenue'] > 435]['visitorid']\n",
    "abnormalUsers = pd.concat([usersWithManyOrders, usersWithExpensiveOrders], axis = 0).drop_duplicates().sort_values()\n",
    "print(abnormalUsers.head(5))\n",
    "print(abnormalUsers.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleAFiltered = pd.concat([ordersByUsersA[np.logical_not(ordersByUsersA['userId'].isin(abnormalUsers))]['orders'],pd.Series(0, index=np.arange(visit[visit['group']=='A']['visits'].sum() - len(ordersByUsersA['orders'])),name='orders')],axis=0)\n",
    "\n",
    "sampleBFiltered = pd.concat([ordersByUsersB[np.logical_not(ordersByUsersB['userId'].isin(abnormalUsers))]['orders'],pd.Series(0, index=np.arange(visit[visit['group']=='B']['visits'].sum() - len(ordersByUsersB['orders'])),name='orders')],axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Testing:**\n",
    "Let's compare the sample means for filtered conversion:\n",
    "1. H0  - the sample means have no difference.\n",
    "1. H1  - the sample means are different.\n",
    "1. alpha - 0.05    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{0:.3f}\".format(stats.mannwhitneyu(sampleAFiltered, sampleBFiltered,alternative= \"two-sided\")[1]))\n",
    "print(\"{0:.3f}\".format(sampleBFiltered.mean()/sampleAFiltered.mean()-1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Conclusion: \n",
    "\n",
    "We have 75 anomalous users in total.The results for conversion after filtering anomalies didn't change as p-value is higher than 0.05, there's no reason to reject the null hypothesis.So, it is not statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Find the statistical significance of the difference in average order size between the groups using the filtered data. Make conclusions and conjectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Testing:**\n",
    "Let's compare the sample means for filtered order size:\n",
    "1. H0  - the sample means have no difference.\n",
    "1. H1  - the sample means are different.\n",
    "1. alpha - 0.05    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{0:.3f}'.format(stats.mannwhitneyu(order[np.logical_and(order['group']=='A',np.logical_not(order['visitorid'].isin(abnormalUsers)))]['revenue'],order[np.logical_and(order['group']=='B',np.logical_not(order['visitorid'].isin(abnormalUsers)))]['revenue'],alternative = \"two-sided\")[1]))\n",
    "\n",
    "print('{0:.3f}'.format(order[np.logical_and(order['group']=='B',np.logical_not(order['visitorid'].isin(abnormalUsers)))]['revenue'].mean()/order[np.logical_and(order['group']=='A',np.logical_not(order['visitorid'].isin(abnormalUsers)))]['revenue'].mean() - 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion:\n",
    "    \n",
    "The p-value of average order size between the groups increased, but now the difference between the segments is 2.7% instead of 25%.The p-value is considerably higher than 0.05, so there's no reason to reject the null hypothesis and conclude that average order size differs between the groups.So, it is not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Make a decision based on the test results. The possible decisions are: 1. Stop the test, consider one of the groups the leader. 2. Stop the test, conclude that there is no difference between the groups. 3. Continue the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion:\n",
    "\n",
    "We didn't get a statistics significant difference between the groups in conversion and in order size.Filtering out the outliers decreased the amount off noise in the data: the variance decreased.So than even 2.7% differnce in order size became statiscally significant.Based on these facts, we can conclude that the test is unsuccessful and should be stopped. There's no use continuing it, since the probability that group B will turn out to be better than group A is almost nonexistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Overall conclusion\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Download the data and prepare it for analysis\n",
    "\n",
    "In the beginning, the datasets have been opened and the general information have been analyzed .There have three dataframe hypotheses,order and visit. The hypotheses data set has 9 rows and 5 columns, The order data set has 1197 rows and 5 columns and The visit data set has 62 rows and 3 columns. There have no missing and duplicate values observed.And all The data types are correct.\n",
    "\n",
    "## Step 2.Prioritizing Hypotheses\n",
    "\n",
    "ICE gives 8,0,7 and 6 as the most promissing hypotheses considering index number.For RICE, the most promissing hypotheses are 7,2,0 and 6 considering index number.However, this time index number 7 is in first place.That's because its reach parameter is 10,where the other hypotheses have lower reach values.Different methods change the hypotheses priorities in different level.Apart from , how strong the hypotheses is, it is important to consider how many users will affect.So 'reach' metric plays a vital role from my understandings.\n",
    "\n",
    "## Step 3. A/B Test Analysis\n",
    "\n",
    "* The cumulative metrics of the B group are consistently higer than those of the A group.Fluctuation in conversion was observed in B groups.It seems like abnormally big orders for B groups affected the results overall. It is most likely to early to make decision based on this metric.\n",
    "\n",
    "* The cumulative value of the segments average purchase size is still fluctuating.It is most likely to early to make decision based on this metric.Or we may need to analyse the outliers that are distorting the results.\n",
    "\n",
    "* At several points the difference between the segments spikes. This means there must be some big orders and outliers!\n",
    "\n",
    "* At the beginning of the test,group A had higher conversion rate but group B gradually gained and established at an almost steady value.The groups were fluctuating around the same value, but then the conversion rate of group A rose before stabilizing, while the conversion rate of group B increased but then stabilized, too.\n",
    "\n",
    "* There are users with 11, 9, 8, 5 orders. That's a lot more than a regular user would place in 4 days.The majority of customers placed only one order. However, a significant share placed two to three apiece.There are indeed a lot of users with two to three orders. The exact share remains unknown, since it's not clear whether we should consider them anomalies or not.\n",
    "\n",
    "* Not more than 5% of users placed more than 2 orders, and 10% of users made more than one. Thus, it would be reasonable to set two or three orders per user as the lower limit for the number of orders and to filter anomalies on that basis.\n",
    "\n",
    "* There are users with 19920 usd average revenue. That's a lot more than a regular user would buy in 4 days.The majority of customers placed order in the average price range less than 2500. The exact share remains unknown, since it's not clear whether we should consider them anomalies or not.\n",
    "\n",
    "* Not more than 5% of users average order price more than 427 Usd, and 10% of users made more than 281 Usd. Thus, it would be reasonable to set 427 Usd per user as the lower limit for the order price and to filter anomalies on that basis.\n",
    "\n",
    "* Based on the raw data, there is no difference between groups A and B. The first row of the output gives us the p-value, 0.017, which is greater than 0.05. So we can't reject the null hypothesis that there's not a statistically significant difference in conversion between the groups. But the relative difference of group B is 13.8% (the second row of the output)\n",
    "\n",
    "* The p-value is considerably higher than 0.05, so there's no reason to reject the null hypothesis and conclude that average order size differs between the groups. Nonetheless, the average order size for group B is smaller than it is for group A and it is almost 25% bigger.So, it is not statistically significant.\n",
    "\n",
    "* We have 75 anomalous users in total.The results for conversion after filtering anomalies didn't change as p-value is higher than 0.05, there's no reason to reject the null hypothesis.So, it is not statistically significant.\n",
    "\n",
    "* We didn't get a statistics significant difference between the groups in conversion and in order size.Filtering out the outliers decreased the amount off noise in the data: the variance decreased.So than even 2.7% differnce in order size became statiscally significant.Based on these facts, we can conclude that the test is unsuccessful and should be stopped. There's no use continuing it, since the probability that group B will turn out to be better than group A is almost nonexistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
